{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13824393,"sourceType":"datasetVersion","datasetId":8803826}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openpyxl\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nMODEL_NAME = \"j-hartmann/emotion-english-distilroberta-base\"\nOUTPUT_DIR = \"./emotion_classifier_jhartmann\"\nSEED = 42\n\n# Note: j-hartmann model uses different labels, we'll map to our 4 categories\n# Original labels: anger, disgust, fear, joy, neutral, sadness, surprise\n# Our labels: Joy, Sadness, Neutral, Anger\nEMOTION_LABELS = {\n    0: \"Joy\",\n    1: \"Sadness\", \n    2: \"Neutral\",\n    3: \"Anger\"\n}\n\n# Improved mapping from j-hartmann labels to our labels\nJHARTMANN_TO_OUR_LABELS = {\n    \"joy\": 0,      # Joy\n    \"sadness\": 1,  # Sadness  \n    \"neutral\": 2,  # Neutral\n    \"anger\": 3,    # Anger\n    \"disgust\": 3,  # Map disgust to Anger (similar negative emotion)\n    \"fear\": 1,     # Map fear to Sadness (negative emotion) \n    \"surprise\": 2  # Map surprise to Neutral (often ambiguous)\n}\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset, ClassLabel\nimport evaluate\nfrom transformers import EvalPrediction\nimport torch.nn.functional as F\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport random\nimport json\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# ============================================================================\n# LOAD DATA\n# ============================================================================\ndf = pd.read_csv('/kaggle/input/emotions1/emotions-dataset.csv').copy()\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.dropna(subset=['content'], inplace=True)\n\nprint(\"=\" * 80)\nprint(\"EMOTION DISTRIBUTION\")\nprint(\"=\" * 80)\nfor label_id, emotion_name in EMOTION_LABELS.items():\n    count = (df['sentiment'] == label_id).sum()\n    pct = (count / len(df)) * 100\n    print(f\"{label_id}: {emotion_name:10s} - {count:5d} samples ({pct:5.2f}%)\")\nprint(f\"\\nTotal: {len(df)} samples\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# SIMPLE TEXT CLEANING - Minimal processing for pre-trained model\n# ============================================================================\ndef clean_text_simple(text):\n    \"\"\"\n    Simple cleaning - the j-hartmann model is already trained on emotional text\n    so we don't need heavy preprocessing\n    \"\"\"\n    text = str(text)\n   \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n   \n    # Clean whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n   \n    return text\n\ndf['cleaned_content'] = df['content'].apply(clean_text_simple)\ndf = df.rename(columns={'sentiment': 'label'})\n\n# Show examples per emotion\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE TEXTS PER EMOTION\")\nprint(\"=\" * 80)\nfor label_id, emotion_name in EMOTION_LABELS.items():\n    print(f\"\\n{emotion_name.upper()} (Label {label_id}):\")\n    samples = df[df['label'] == label_id]['cleaned_content'].head(3).tolist()\n    for i, sample in enumerate(samples, 1):\n        print(f\" {i}. {sample[:80]}...\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# CLASS WEIGHT COMPUTATION\n# ============================================================================\nnum_labels = len(EMOTION_LABELS)\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(df['label']),\n    y=df['label']\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CLASS WEIGHTS (for imbalanced data)\")\nprint(\"=\" * 80)\nfor i, weight in enumerate(class_weights):\n    print(f\"{EMOTION_LABELS[i]:10s} (Label {i}): {weight:.3f}\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# TOKENIZATION - Store test set texts for later analysis\n# ============================================================================\n\n# Create a copy of the dataframe with original indices for test set reconstruction\ndf_with_index = df.reset_index().rename(columns={'index': 'original_index'})\n\nhf_dataset = Dataset.from_pandas(df_with_index[['cleaned_content', 'label', 'original_index']], preserve_index=False)\n\n# Cast label to ClassLabel for stratification\nhf_dataset = hf_dataset.cast_column('label', ClassLabel(names=[EMOTION_LABELS[i] for i in range(num_labels)]))\n\n# Load tokenizer from j-hartmann model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"cleaned_content\"],\n        truncation=True,\n        max_length=128,\n        padding=False # Dynamic padding via data collator\n    )\n\ntokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=['cleaned_content'])\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Stratified split to maintain emotion distribution\nsplit_dataset = tokenized_dataset.train_test_split(\n    test_size=0.2,\n    seed=SEED,\n    stratify_by_column='label'\n)\n\ntest_valid_split = split_dataset['test'].train_test_split(\n    test_size=0.5,\n    seed=SEED,\n    stratify_by_column='label'\n)\n\ntrain_dataset = split_dataset['train']\nvalidation_dataset = test_valid_split['train']\ntest_dataset = test_valid_split['test']\n\nprint(f\"\\n‚úì Dataset sizes - Train: {len(train_dataset)} | Val: {len(validation_dataset)} | Test: {len(test_dataset)}\")\n\n# Verify stratification\nprint(\"\\nValidation set distribution:\")\nfor label in range(num_labels):\n    count = sum(1 for x in validation_dataset if x['label'] == label)\n    pct = (count / len(validation_dataset)) * 100\n    print(f\" {EMOTION_LABELS[label]:10s}: {count:4d} ({pct:5.2f}%)\")\n\n# Store test set texts for error analysis\ntest_indices = test_dataset['original_index']\ntest_texts = df.loc[test_indices, 'cleaned_content'].tolist()\n\n# ============================================================================\n# MODEL LOADING - Using j-hartmann pre-trained emotion model\n# ============================================================================\nprint(f\"\\nLoading pre-trained emotion model: {MODEL_NAME}\")\nprint(\"This model is already fine-tuned on emotion classification!\")\n\n# Load the model - it already has emotion classification capabilities\n# We just need to adjust the classifier for our 4 classes instead of 7\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=num_labels,\n    ignore_mismatched_sizes=True # Important: allows loading with different number of labels\n)\n\nprint(\"‚úì Model loaded successfully!\")\nprint(f\"Original model has 7 emotions, adapted to our {num_labels} emotions\")\n\n# ============================================================================\n# WEIGHTED TRAINER\n# ============================================================================\nclass WeightedTrainer(Trainer):\n    \"\"\"\n    Trainer with class weights for handling imbalance\n    \"\"\"\n    def __init__(self, class_weights=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n       \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n       \n        # Standard cross-entropy with class weights\n        if self.class_weights is not None:\n            device = labels.device\n            weight_tensor = torch.tensor(self.class_weights, dtype=torch.float).to(device)\n            loss = F.cross_entropy(logits, labels, weight=weight_tensor)\n        else:\n            loss = F.cross_entropy(logits, labels)\n       \n        return (loss, outputs) if return_outputs else loss\n\n# ============================================================================\n# METRICS\n# ============================================================================\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n   \n    # Overall metrics\n    accuracy = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n    f1_macro = f1_metric.compute(predictions=preds, references=labels, average='macro')['f1']\n    f1_weighted = f1_metric.compute(predictions=preds, references=labels, average='weighted')['f1']\n   \n    # Per-class F1 scores\n    f1_per_class = f1_metric.compute(predictions=preds, references=labels, average=None)['f1']\n   \n    metrics = {\n        \"accuracy\": accuracy,\n        \"f1_macro\": f1_macro,\n        \"f1_weighted\": f1_weighted,\n    }\n   \n    # Add per-emotion F1 scores\n    for i, emotion in EMOTION_LABELS.items():\n        metrics[f\"f1_{emotion.lower()}\"] = f1_per_class[i]\n   \n    return metrics\n\n# ============================================================================\n# TRAINING ARGUMENTS - Optimized for fine-tuning pre-trained model\n# ============================================================================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n   \n    # Training hyperparameters - fewer epochs needed since model is pre-trained on emotions\n    num_train_epochs=4, # Reduced because we're fine-tuning an already emotion-trained model\n    per_device_train_batch_size=32, # Increased batch size\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5, # Standard fine-tuning LR\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n   \n    # Evaluation strategy\n    eval_strategy=\"epoch\", # Evaluate every epoch\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n   \n    # Model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n   \n    # Logging\n    logging_dir='./logs',\n    logging_steps=50,\n   \n    # Performance\n    fp16=True,\n    dataloader_num_workers=2,\n    seed=SEED,\n   \n    report_to=\"none\"\n)\n\n# ============================================================================\n# INITIALIZE TRAINER\n# ============================================================================\ntrainer = WeightedTrainer(\n    class_weights=class_weights,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # Less patience needed\n)\n\n# ============================================================================\n# TRAIN\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINE-TUNING PRE-TRAINED EMOTION CLASSIFIER\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(\"Strategy: Transfer Learning + Class Weights\")\nprint(\"=\" * 80)\n\ntrainer.train()\n\n# ============================================================================\n# FINAL EVALUATION\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL EVALUATION ON TEST SET\")\nprint(\"=\" * 80)\n\ntest_results = trainer.evaluate(test_dataset)\nprint(\"\\n--- Test Set Performance ---\")\nprint(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (Macro): {test_results['eval_f1_macro']:.4f}\")\nprint(f\"F1 (Weight): {test_results['eval_f1_weighted']:.4f}\")\nprint(\"\\nPer-Emotion F1 Scores:\")\nfor emotion in EMOTION_LABELS.values():\n    key = f\"eval_f1_{emotion.lower()}\"\n    print(f\" {emotion:10s}: {test_results[key]:.4f}\")\n\n# ============================================================================\n# CONFUSION MATRIX & CLASSIFICATION REPORT\n# ============================================================================\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=1)\ntrue_labels = predictions.label_ids\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\" * 80)\nprint(\" \", \" \".join([f\"{EMOTION_LABELS[i]:8s}\" for i in range(num_labels)]))\ncm = confusion_matrix(true_labels, preds)\nfor i, row in enumerate(cm):\n    print(f\"{EMOTION_LABELS[i]:10s}: \", \" \".join([f\"{val:8d}\" for val in row]))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\" * 80)\nemotion_names = [EMOTION_LABELS[i] for i in range(num_labels)]\nprint(classification_report(true_labels, preds, target_names=emotion_names, digits=4))\n\n# ============================================================================\n# ERROR ANALYSIS - FIXED VERSION\n# ============================================================================\ndef analyze_errors(true_labels, preds, test_texts):\n    \"\"\"Analyze misclassifications to understand model weaknesses\"\"\"\n    misclassified = []\n    \n    for i, (true, pred) in enumerate(zip(true_labels, preds)):\n        if true != pred:\n            text_sample = test_texts[i]  # Use stored test texts\n               \n            misclassified.append({\n                'text': text_sample,\n                'true_label': true,\n                'pred_label': pred,\n                'true_emotion': EMOTION_LABELS[true],\n                'pred_emotion': EMOTION_LABELS[pred]\n            })\n   \n    # Count error types\n    error_matrix = np.zeros((num_labels, num_labels))\n    for error in misclassified:\n        error_matrix[error['true_label'], error['pred_label']] += 1\n   \n    print(\"\\n\" + \"=\" * 80)\n    print(\"ERROR ANALYSIS\")\n    print(\"=\" * 80)\n    print(f\"Total misclassified: {len(misclassified)} ({len(misclassified)/len(true_labels):.1%})\")\n    print(\"\\nMost common misclassifications:\")\n    \n    error_counts = []\n    for true_idx in range(num_labels):\n        for pred_idx in range(num_labels):\n            if true_idx != pred_idx and error_matrix[true_idx, pred_idx] > 0:\n                count = error_matrix[true_idx, pred_idx]\n                percentage = (count / len(misclassified)) * 100\n                error_counts.append((true_idx, pred_idx, count, percentage))\n    \n    # Sort by count descending\n    error_counts.sort(key=lambda x: x[2], reverse=True)\n    \n    for true_idx, pred_idx, count, percentage in error_counts[:10]:  # Show top 10\n        print(f\" {EMOTION_LABELS[true_idx]} ‚Üí {EMOTION_LABELS[pred_idx]}: {count:.0f} errors ({percentage:.1f}%)\")\n    \n    # Show some example errors\n    print(f\"\\nExample misclassifications:\")\n    for i, error in enumerate(misclassified[:5]):\n        print(f\"\\n {i+1}. '{error['text'][:100]}...'\")\n        print(f\"    True: {error['true_emotion']} ‚Üí Pred: {error['pred_emotion']}\")\n   \n    return misclassified, error_matrix\n\nmisclassified, error_matrix = analyze_errors(true_labels, preds, test_texts)\n\n# ============================================================================\n# COMPARE WITH BASELINE (Zero-shot with original j-hartmann model) - FIXED\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BASELINE COMPARISON: Zero-shot with original model\")\nprint(\"=\" * 80)\n\n# Load original model for comparison\noriginal_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\noriginal_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\noriginal_model.eval()\n\ndef predict_with_original_model(texts):\n    \"\"\"Get predictions using original j-hartmann model\"\"\"\n    inputs = original_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    with torch.no_grad():\n        outputs = original_model(**inputs)\n    probs = F.softmax(outputs.logits, dim=-1)\n    preds = torch.argmax(probs, dim=1)\n    return preds.numpy(), probs.numpy()\n\n# Test on actual test set samples (first 100 for speed)\nsample_texts = test_texts[:100]\nsample_true_labels = true_labels[:100]\n\noriginal_preds, original_probs = predict_with_original_model(sample_texts)\n\n# Map original predictions to our labels\nmapped_preds = []\nfor pred in original_preds:\n    # j-hartmann model label names\n    original_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n    predicted_emotion = original_labels[pred]\n    mapped_label = JHARTMANN_TO_OUR_LABELS.get(predicted_emotion, 2) # default to Neutral\n    mapped_preds.append(mapped_label)\n\n# Calculate baseline accuracy\nbaseline_accuracy = np.mean(np.array(mapped_preds) == np.array(sample_true_labels))\nprint(f\"Baseline (zero-shot) accuracy on test samples: {baseline_accuracy:.2%}\")\nprint(f\"Our fine-tuned model accuracy: {test_results['eval_accuracy']:.2%}\")\nprint(f\"Improvement: {test_results['eval_accuracy'] - baseline_accuracy:+.2%}\")\n\n# ============================================================================\n# INFERENCE WITH EXPLANATIONS\n# ============================================================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\ndef predict_with_confidence(text, show_all_probs=False):\n    \"\"\"Predict emotion with confidence scores\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n    inputs = inputs.to(device)\n   \n    with torch.no_grad():\n        outputs = model(**inputs)\n   \n    probs = F.softmax(outputs.logits, dim=-1)[0]\n    pred_label = torch.argmax(probs).item()\n   \n    if show_all_probs:\n        print(f\"\\nText: {text}\")\n        print(\"Probabilities:\")\n        for i, prob in enumerate(probs):\n            print(f\" {EMOTION_LABELS[i]:10s}: {prob.item():.2%}\")\n        print(f\"‚Üí Predicted: {EMOTION_LABELS[pred_label]}\")\n   \n    return pred_label, probs[pred_label].item()\n\n# Test on diverse samples from test set\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INFERENCE ON DIVERSE TEST SAMPLES\")\nprint(\"=\" * 80)\n\n# Get samples from each emotion in test set\ntest_samples = []\nfor emotion_id in range(num_labels):\n    # Find indices in test set for this emotion\n    emotion_indices = np.where(true_labels == emotion_id)[0]\n    if len(emotion_indices) >= 2:\n        # Take first 2 samples of this emotion from test set\n        for idx in emotion_indices[:2]:\n            test_samples.append((test_texts[idx], emotion_id))\n\ncorrect_predictions = 0\ntotal_predictions = len(test_samples)\n\nprint(f\"Testing on {total_predictions} diverse samples from test set:\")\nfor text, true_label in test_samples:\n    pred_label, confidence = predict_with_confidence(text, show_all_probs=False)\n   \n    status = \"‚úÖ\" if pred_label == true_label else \"‚ùå\"\n    if pred_label == true_label:\n        correct_predictions += 1\n   \n    true_emotion = EMOTION_LABELS[true_label]\n    pred_emotion = EMOTION_LABELS[pred_label]\n   \n    print(f\"\\n{status} '{text[:70]}...'\")\n    print(f\" True: {true_emotion:10s} | Pred: {pred_emotion:10s} | Conf: {confidence:.1%}\")\n\nprint(f\"\\nDiverse sample accuracy: {correct_predictions}/{total_predictions} = {correct_predictions/total_predictions:.1%}\")\n\n# ============================================================================\n# SAVE MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAVING FINE-TUNED MODEL\")\nprint(\"=\" * 80)\n\ntrainer.save_model(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\n# Save configuration\nconfig_info = {\n    \"emotion_labels\": EMOTION_LABELS,\n    \"base_model\": MODEL_NAME,\n    \"fine_tuned_on\": \"4-emotion dataset\",\n    \"performance\": {\n        \"accuracy\": test_results['eval_accuracy'],\n        \"f1_macro\": test_results['eval_f1_macro'],\n        \"f1_weighted\": test_results['eval_f1_weighted']\n    }\n}\n\nwith open(f\"{OUTPUT_DIR}/final_model/model_config.json\", 'w') as f:\n    json.dump(config_info, f, indent=2)\n\nprint(f\"‚úì Model saved to: {OUTPUT_DIR}/final_model\")\nprint(f\"‚úì Model configuration saved\")\nprint(\"=\" * 80)\nprint(\"\\nüéâ FINE-TUNING COMPLETE!\")\nprint(f\"Final Test Accuracy: {test_results['eval_accuracy']:.2%}\")\nprint(f\"Final Test F1 (Macro): {test_results['eval_f1_macro']:.2%}\")\n\n# ============================================================================\n# PERFORMANCE SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PERFORMANCE SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"6. ‚úì Achieved performance: {test_results['eval_accuracy']:.2%} accuracy\")\nprint(\"\\nModel Strengths:\")\nprint(f\" - Best at detecting Anger (F1: {test_results['eval_f1_anger']:.2%})\")\nprint(f\" - Good Neutral detection (F1: {test_results['eval_f1_neutral']:.2%})\")\nprint(\"\\nAreas for Improvement:\")\nprint(f\" - Joy detection could be improved (F1: {test_results['eval_f1_joy']:.2%})\")\nprint(\" - Some confusion between Joy/Sadness/Neutral\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T06:59:54.641995Z","iopub.execute_input":"2025-11-22T06:59:54.642899Z","iopub.status.idle":"2025-11-22T07:06:08.272890Z","shell.execute_reply.started":"2025-11-22T06:59:54.642869Z","shell.execute_reply":"2025-11-22T07:06:08.271950Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n================================================================================\nEMOTION DISTRIBUTION\n================================================================================\n0: Joy        -  9211 samples (26.27%)\n1: Sadness    -  9530 samples (27.18%)\n2: Neutral    -  6412 samples (18.29%)\n3: Anger      -  9913 samples (28.27%)\n\nTotal: 35066 samples\n================================================================================\n\n================================================================================\nSAMPLE TEXTS PER EMOTION\n================================================================================\n\nJOY (Label 0):\n 1. vene2ia great...\n 2. mrcartersnurse congratulations for your mom for tomorrow buenas noches...\n 3. ddlovato oh i see thanks for replying anyway how are you...\n\nSADNESS (Label 1):\n 1. just finished watching quotmarley and mequot...\n 2. msignorile weather sucks up here...\n 3. sasss09 hahaha sadly this ones supposed to be done individually so no chance of ...\n\nNEUTRAL (Label 2):\n 1. sadknob right now if be happy to win a packet of salt n vinegar crisps or a new ...\n 2. ok im out of here for now just popped in to say hi and check on things ill proba...\n 3. just finished my 1st new song soon on youtube keeping you updated...\n\nANGER (Label 3):\n 1. tanisha found herself in an outrageous situation...\n 2. ok free people skirt hide and seeks over...\n 3. the toys r us advert makes me want to scream...\n================================================================================\n\n================================================================================\nCLASS WEIGHTS (for imbalanced data)\n================================================================================\nJoy        (Label 0): 0.952\nSadness    (Label 1): 0.920\nNeutral    (Label 2): 1.367\nAnger      (Label 3): 0.884\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/35066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96e2cca954944609988b53de84cfc9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa6f03da1bc84d28a5a64910b9a3481d"}},"metadata":{}},{"name":"stdout","text":"\n‚úì Dataset sizes - Train: 28052 | Val: 3507 | Test: 3507\n\nValidation set distribution:\n Joy       :  921 (26.26%)\n Sadness   :  953 (27.17%)\n Neutral   :  642 (18.31%)\n Anger     :  991 (28.26%)\n\nLoading pre-trained emotion model: j-hartmann/emotion-english-distilroberta-base\nThis model is already fine-tuned on emotion classification!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([4]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úì Model loaded successfully!\nOriginal model has 7 emotions, adapted to our 4 emotions\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/2695968743.py:200: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nPRE-TRAINED MODEL CAPABILITIES\n================================================================================\nThe j-hartmann model is already trained on:\n - 7 emotions: anger, disgust, fear, joy, neutral, sadness, surprise\n - Large emotional dataset\n - Good understanding of emotional language patterns\nWe are adapting it to our 4 emotion categories\n================================================================================\n\n================================================================================\nFINE-TUNING PRE-TRAINED EMOTION CLASSIFIER\nModel: j-hartmann/emotion-english-distilroberta-base\nStrategy: Transfer Learning + Class Weights\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1756' max='1756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1756/1756 05:49, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n      <th>F1 Weighted</th>\n      <th>F1 Joy</th>\n      <th>F1 Sadness</th>\n      <th>F1 Neutral</th>\n      <th>F1 Anger</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.647200</td>\n      <td>0.612753</td>\n      <td>0.765041</td>\n      <td>0.761396</td>\n      <td>0.764135</td>\n      <td>0.697124</td>\n      <td>0.753933</td>\n      <td>0.747573</td>\n      <td>0.846954</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.547300</td>\n      <td>0.592762</td>\n      <td>0.776732</td>\n      <td>0.773817</td>\n      <td>0.777002</td>\n      <td>0.714609</td>\n      <td>0.766355</td>\n      <td>0.754500</td>\n      <td>0.859803</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.492800</td>\n      <td>0.592416</td>\n      <td>0.775307</td>\n      <td>0.772303</td>\n      <td>0.775411</td>\n      <td>0.706607</td>\n      <td>0.765027</td>\n      <td>0.755031</td>\n      <td>0.862545</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.440900</td>\n      <td>0.601648</td>\n      <td>0.776732</td>\n      <td>0.774041</td>\n      <td>0.777021</td>\n      <td>0.712206</td>\n      <td>0.766385</td>\n      <td>0.757337</td>\n      <td>0.860237</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nFINAL EVALUATION ON TEST SET\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Test Set Performance ---\nAccuracy: 0.7821\nF1 (Macro): 0.7796\nF1 (Weight): 0.7823\n\nPer-Emotion F1 Scores:\n Joy       : 0.7230\n Sadness   : 0.7564\n Neutral   : 0.7662\n Anger     : 0.8728\n\n================================================================================\nCONFUSION MATRIX\n================================================================================\n  Joy      Sadness  Neutral  Anger   \nJoy       :       646      122      116       37\nSadness   :       115      725       56       57\nNeutral   :        64       43      521       13\nAnger     :        41       74       26      851\n\n================================================================================\nDETAILED CLASSIFICATION REPORT\n================================================================================\n              precision    recall  f1-score   support\n\n         Joy     0.7460    0.7014    0.7230       921\n     Sadness     0.7521    0.7608    0.7564       953\n     Neutral     0.7246    0.8128    0.7662       641\n       Anger     0.8883    0.8579    0.8728       992\n\n    accuracy                         0.7821      3507\n   macro avg     0.7777    0.7832    0.7796      3507\nweighted avg     0.7840    0.7821    0.7823      3507\n\n\n================================================================================\nERROR ANALYSIS\n================================================================================\nTotal misclassified: 764 (21.8%)\n\nMost common misclassifications:\n Joy ‚Üí Sadness: 122 errors (16.0%)\n Joy ‚Üí Neutral: 116 errors (15.2%)\n Sadness ‚Üí Joy: 115 errors (15.1%)\n Anger ‚Üí Sadness: 74 errors (9.7%)\n Neutral ‚Üí Joy: 64 errors (8.4%)\n Sadness ‚Üí Anger: 57 errors (7.5%)\n Sadness ‚Üí Neutral: 56 errors (7.3%)\n Neutral ‚Üí Sadness: 43 errors (5.6%)\n Anger ‚Üí Joy: 41 errors (5.4%)\n Joy ‚Üí Anger: 37 errors (4.8%)\n\nExample misclassifications:\n\n 1. 'hmmosaka last show todayvery sad i can decode ur msg haha cant wait till u get 2 sydney d i missed o...'\n    True: Neutral ‚Üí Pred: Sadness\n\n 2. 'im sunburnt on my arms and i have burnt my mouth and some skin is coming off...'\n    True: Sadness ‚Üí Pred: Anger\n\n 3. 'torithompson u may need this pilli think u are crazyhere take on of mine...'\n    True: Anger ‚Üí Pred: Neutral\n\n 4. 'having my hair dyed today ugh im bored still tired from friday lol swear down bossman...'\n    True: Joy ‚Üí Pred: Sadness\n\n 5. 'idk why im so hyper im jumping everyhere ugh lets let it be friday sweeney todd then cinco de mayo i...'\n    True: Sadness ‚Üí Pred: Anger\n\n================================================================================\nBASELINE COMPARISON: Zero-shot with original model\n================================================================================\nBaseline (zero-shot) accuracy on test samples: 48.00%\nOur fine-tuned model accuracy: 78.21%\nImprovement: +30.21%\n\n================================================================================\nINFERENCE ON DIVERSE TEST SAMPLES\n================================================================================\nTesting on 8 diverse samples from test set:\n\n‚úÖ '3wordsaftersex my turn yet...'\n True: Joy        | Pred: Joy        | Conf: 77.9%\n\n‚úÖ 'i have turned into a fast food whore...'\n True: Joy        | Pred: Joy        | Conf: 47.0%\n\n‚úÖ 'lulion07 im praying for you sorry to hear about your bro man...'\n True: Sadness    | Pred: Sadness    | Conf: 99.2%\n\n‚úÖ 'i feel sicklike dont wanna get out of bed be bothered dont go 2 work t...'\n True: Sadness    | Pred: Sadness    | Conf: 67.5%\n\n‚ùå 'hmmosaka last show todayvery sad i can decode ur msg haha cant wait ti...'\n True: Neutral    | Pred: Sadness    | Conf: 98.0%\n\n‚úÖ 'divinediva1 norwood house party haaaaaa yaaaaaaay smiles...'\n True: Neutral    | Pred: Neutral    | Conf: 98.4%\n\n‚úÖ 'gooooodnight i fully gave up on my english pride and prejudice love th...'\n True: Anger      | Pred: Anger      | Conf: 86.1%\n\n‚ùå 'torithompson u may need this pilli think u are crazyhere take on of mi...'\n True: Anger      | Pred: Neutral    | Conf: 48.3%\n\nDiverse sample accuracy: 6/8 = 75.0%\n\n================================================================================\nSAVING FINE-TUNED MODEL\n================================================================================\n‚úì Model saved to: ./emotion_classifier_jhartmann/final_model\n‚úì Model configuration saved\n================================================================================\n\nüéâ FINE-TUNING COMPLETE!\nFinal Test Accuracy: 78.21%\nFinal Test F1 (Macro): 77.96%\n\n================================================================================\nPERFORMANCE SUMMARY\n================================================================================\n6. ‚úì Achieved performance: 78.21% accuracy\n\nModel Strengths:\n - Best at detecting Anger (F1: 87.28%)\n - Good Neutral detection (F1: 76.62%)\n\nAreas for Improvement:\n - Joy detection could be improved (F1: 72.30%)\n - Some confusion between Joy/Sadness/Neutral\n================================================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install openpyxl\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nMODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\nOUTPUT_DIR = \"./sentiment_classifier_cardiffnlp\"\nSEED = 42\n\n# Note: cardiffnlp model uses negative, neutral, positive labels\n# We'll map to our 4 categories: Joy, Sadness, Neutral, Anger\nEMOTION_LABELS = {\n    0: \"Joy\",\n    1: \"Sadness\", \n    2: \"Neutral\",\n    3: \"Anger\"\n}\n\n# Mapping from cardiffnlp sentiment labels to our emotion labels\nCARDIFFNLP_TO_OUR_LABELS = {\n    \"positive\": 0,   # Joy\n    \"neutral\": 2,    # Neutral\n    \"negative\": 1,   # Sadness (we'll handle Anger separately)\n}\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset, ClassLabel\nimport evaluate\nfrom transformers import EvalPrediction\nimport torch.nn.functional as F\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport random\nimport json\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# ============================================================================\n# LOAD DATA\n# ============================================================================\ndf = pd.read_csv('/kaggle/input/emotions1/emotions-dataset.csv').copy()\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.dropna(subset=['content'], inplace=True)\n\nprint(\"=\" * 80)\nprint(\"EMOTION DISTRIBUTION\")\nprint(\"=\" * 80)\nfor label_id, emotion_name in EMOTION_LABELS.items():\n    count = (df['sentiment'] == label_id).sum()\n    pct = (count / len(df)) * 100\n    print(f\"{label_id}: {emotion_name:10s} - {count:5d} samples ({pct:5.2f}%)\")\nprint(f\"\\nTotal: {len(df)} samples\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# TEXT CLEANING - Optimized for Twitter model\n# ============================================================================\ndef clean_text_twitter(text):\n    \"\"\"\n    Cleaning optimized for Twitter-trained model\n    \"\"\"\n    text = str(text)\n   \n    # Remove URLs but keep other Twitter elements\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Clean extra whitespace but preserve emojis and hashtags\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Remove user mentions if they don't add context\n    text = re.sub(r'@\\w+', '', text)\n    \n    return text.strip()\n\ndf['cleaned_content'] = df['content'].apply(clean_text_twitter)\ndf = df.rename(columns={'sentiment': 'label'})\n\n# Show examples per emotion\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE TEXTS PER EMOTION\")\nprint(\"=\" * 80)\nfor label_id, emotion_name in EMOTION_LABELS.items():\n    print(f\"\\n{emotion_name.upper()} (Label {label_id}):\")\n    samples = df[df['label'] == label_id]['cleaned_content'].head(3).tolist()\n    for i, sample in enumerate(samples, 1):\n        print(f\" {i}. {sample[:80]}...\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# CLASS WEIGHT COMPUTATION\n# ============================================================================\nnum_labels = len(EMOTION_LABELS)\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(df['label']),\n    y=df['label']\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CLASS WEIGHTS (for imbalanced data)\")\nprint(\"=\" * 80)\nfor i, weight in enumerate(class_weights):\n    print(f\"{EMOTION_LABELS[i]:10s} (Label {i}): {weight:.3f}\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# TOKENIZATION - Store test set texts for later analysis\n# ============================================================================\n\n# Create a copy of the dataframe with original indices for test set reconstruction\ndf_with_index = df.reset_index().rename(columns={'index': 'original_index'})\n\nhf_dataset = Dataset.from_pandas(df_with_index[['cleaned_content', 'label', 'original_index']], preserve_index=False)\n\n# Cast label to ClassLabel for stratification\nhf_dataset = hf_dataset.cast_column('label', ClassLabel(names=[EMOTION_LABELS[i] for i in range(num_labels)]))\n\n# Load tokenizer from cardiffnlp model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"cleaned_content\"],\n        truncation=True,\n        max_length=128,\n        padding=False # Dynamic padding via data collator\n    )\n\ntokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=['cleaned_content'])\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Stratified split to maintain emotion distribution\nsplit_dataset = tokenized_dataset.train_test_split(\n    test_size=0.2,\n    seed=SEED,\n    stratify_by_column='label'\n)\n\ntest_valid_split = split_dataset['test'].train_test_split(\n    test_size=0.5,\n    seed=SEED,\n    stratify_by_column='label'\n)\n\ntrain_dataset = split_dataset['train']\nvalidation_dataset = test_valid_split['train']\ntest_dataset = test_valid_split['test']\n\nprint(f\"\\n‚úì Dataset sizes - Train: {len(train_dataset)} | Val: {len(validation_dataset)} | Test: {len(test_dataset)}\")\n\n# Verify stratification\nprint(\"\\nValidation set distribution:\")\nfor label in range(num_labels):\n    count = sum(1 for x in validation_dataset if x['label'] == label)\n    pct = (count / len(validation_dataset)) * 100\n    print(f\" {EMOTION_LABELS[label]:10s}: {count:4d} ({pct:5.2f}%)\")\n\n# Store test set texts for error analysis\ntest_indices = test_dataset['original_index']\ntest_texts = df.loc[test_indices, 'cleaned_content'].tolist()\n\n# ============================================================================\n# MODEL LOADING - Using cardiffnlp Twitter sentiment model\n# ============================================================================\nprint(f\"\\nLoading Twitter sentiment model: {MODEL_NAME}\")\nprint(\"This model is pre-trained on Twitter data for sentiment analysis!\")\n\n# Load the model - it has sentiment classification capabilities\n# We need to adjust the classifier for our 4 emotion classes\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=num_labels,\n    ignore_mismatched_sizes=True # Important: allows loading with different number of labels\n)\n\nprint(\"‚úì Twitter sentiment model loaded successfully!\")\nprint(f\"Original model has 3 sentiment classes, adapted to our {num_labels} emotion classes\")\n\n# ============================================================================\n# WEIGHTED TRAINER\n# ============================================================================\nclass WeightedTrainer(Trainer):\n    \"\"\"\n    Trainer with class weights for handling imbalance\n    \"\"\"\n    def __init__(self, class_weights=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n       \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n       \n        # Standard cross-entropy with class weights\n        if self.class_weights is not None:\n            device = labels.device\n            weight_tensor = torch.tensor(self.class_weights, dtype=torch.float).to(device)\n            loss = F.cross_entropy(logits, labels, weight=weight_tensor)\n        else:\n            loss = F.cross_entropy(logits, labels)\n       \n        return (loss, outputs) if return_outputs else loss\n\n# ============================================================================\n# METRICS\n# ============================================================================\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n   \n    # Overall metrics\n    accuracy = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n    f1_macro = f1_metric.compute(predictions=preds, references=labels, average='macro')['f1']\n    f1_weighted = f1_metric.compute(predictions=preds, references=labels, average='weighted')['f1']\n   \n    # Per-class F1 scores\n    f1_per_class = f1_metric.compute(predictions=preds, references=labels, average=None)['f1']\n   \n    metrics = {\n        \"accuracy\": accuracy,\n        \"f1_macro\": f1_macro,\n        \"f1_weighted\": f1_weighted,\n    }\n   \n    # Add per-emotion F1 scores\n    for i, emotion in EMOTION_LABELS.items():\n        metrics[f\"f1_{emotion.lower()}\"] = f1_per_class[i]\n   \n    return metrics\n\n# ============================================================================\n# TRAINING ARGUMENTS - Optimized for Twitter model\n# ============================================================================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n   \n    # Training hyperparameters\n    num_train_epochs=5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n   \n    # Evaluation strategy\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n   \n    # Model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n   \n    # Logging\n    logging_dir='./logs',\n    logging_steps=50,\n   \n    # Performance\n    fp16=True,\n    dataloader_num_workers=2,\n    seed=SEED,\n   \n    report_to=\"none\"\n)\n\n# ============================================================================\n# INITIALIZE TRAINER\n# ============================================================================\ntrainer = WeightedTrainer(\n    class_weights=class_weights,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n# ============================================================================\n# TRAIN\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINE-TUNING TWITTER SENTIMENT MODEL FOR EMOTION CLASSIFICATION\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(\"Strategy: Transfer Learning from Twitter Sentiment + Class Weights\")\nprint(\"=\" * 80)\n\ntrainer.train()\n\n# ============================================================================\n# FINAL EVALUATION\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL EVALUATION ON TEST SET\")\nprint(\"=\" * 80)\n\ntest_results = trainer.evaluate(test_dataset)\nprint(\"\\n--- Test Set Performance ---\")\nprint(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (Macro): {test_results['eval_f1_macro']:.4f}\")\nprint(f\"F1 (Weight): {test_results['eval_f1_weighted']:.4f}\")\nprint(\"\\nPer-Emotion F1 Scores:\")\nfor emotion in EMOTION_LABELS.values():\n    key = f\"eval_f1_{emotion.lower()}\"\n    print(f\" {emotion:10s}: {test_results[key]:.4f}\")\n\n# ============================================================================\n# CONFUSION MATRIX & CLASSIFICATION REPORT\n# ============================================================================\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=1)\ntrue_labels = predictions.label_ids\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\" * 80)\nprint(\" \", \" \".join([f\"{EMOTION_LABELS[i]:8s}\" for i in range(num_labels)]))\ncm = confusion_matrix(true_labels, preds)\nfor i, row in enumerate(cm):\n    print(f\"{EMOTION_LABELS[i]:10s}: \", \" \".join([f\"{val:8d}\" for val in row]))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\" * 80)\nemotion_names = [EMOTION_LABELS[i] for i in range(num_labels)]\nprint(classification_report(true_labels, preds, target_names=emotion_names, digits=4))\n\n# ============================================================================\n# ERROR ANALYSIS\n# ============================================================================\ndef analyze_errors(true_labels, preds, test_texts):\n    \"\"\"Analyze misclassifications to understand model weaknesses\"\"\"\n    misclassified = []\n    \n    for i, (true, pred) in enumerate(zip(true_labels, preds)):\n        if true != pred:\n            text_sample = test_texts[i]\n               \n            misclassified.append({\n                'text': text_sample,\n                'true_label': true,\n                'pred_label': pred,\n                'true_emotion': EMOTION_LABELS[true],\n                'pred_emotion': EMOTION_LABELS[pred]\n            })\n   \n    # Count error types\n    error_matrix = np.zeros((num_labels, num_labels))\n    for error in misclassified:\n        error_matrix[error['true_label'], error['pred_label']] += 1\n   \n    print(\"\\n\" + \"=\" * 80)\n    print(\"ERROR ANALYSIS\")\n    print(\"=\" * 80)\n    print(f\"Total misclassified: {len(misclassified)} ({len(misclassified)/len(true_labels):.1%})\")\n    print(\"\\nMost common misclassifications:\")\n    \n    error_counts = []\n    for true_idx in range(num_labels):\n        for pred_idx in range(num_labels):\n            if true_idx != pred_idx and error_matrix[true_idx, pred_idx] > 0:\n                count = error_matrix[true_idx, pred_idx]\n                percentage = (count / len(misclassified)) * 100\n                error_counts.append((true_idx, pred_idx, count, percentage))\n    \n    # Sort by count descending\n    error_counts.sort(key=lambda x: x[2], reverse=True)\n    \n    for true_idx, pred_idx, count, percentage in error_counts[:10]:\n        print(f\" {EMOTION_LABELS[true_idx]} ‚Üí {EMOTION_LABELS[pred_idx]}: {count:.0f} errors ({percentage:.1f}%)\")\n    \n    # Show some example errors\n    print(f\"\\nExample misclassifications:\")\n    for i, error in enumerate(misclassified[:5]):\n        print(f\"\\n {i+1}. '{error['text'][:100]}...'\")\n        print(f\"    True: {error['true_emotion']} ‚Üí Pred: {error['pred_emotion']}\")\n   \n    return misclassified, error_matrix\n\nmisclassified, error_matrix = analyze_errors(true_labels, preds, test_texts)\n\n# ============================================================================\n# COMPARE WITH BASELINE (Zero-shot with original cardiffnlp model)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BASELINE COMPARISON: Zero-shot with original Twitter model\")\nprint(\"=\" * 80)\n\n# Load original model for comparison\noriginal_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\noriginal_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\noriginal_model.eval()\n\ndef predict_with_original_model(texts):\n    \"\"\"Get predictions using original cardiffnlp model\"\"\"\n    inputs = original_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    with torch.no_grad():\n        outputs = original_model(**inputs)\n    probs = F.softmax(outputs.logits, dim=-1)\n    preds = torch.argmax(probs, dim=1)\n    return preds.numpy(), probs.numpy()\n\n# Test on actual test set samples (first 100 for speed)\nsample_texts = test_texts[:100]\nsample_true_labels = true_labels[:100]\n\noriginal_preds, original_probs = predict_with_original_model(sample_texts)\n\n# Map original predictions to our labels\n# The cardiffnlp model uses: 0 -> negative, 1 -> neutral, 2 -> positive\nmapped_preds = []\nfor pred in original_preds:\n    if pred == 2:  # positive -> Joy\n        mapped_preds.append(0)\n    elif pred == 1:  # neutral -> Neutral\n        mapped_preds.append(2)\n    elif pred == 0:  # negative -> need to distinguish between Sadness and Anger\n        # For baseline, we'll map all negative to Sadness\n        # This is a simplification - in practice you might want more sophisticated mapping\n        mapped_preds.append(1)\n\n# Calculate baseline accuracy\nbaseline_accuracy = np.mean(np.array(mapped_preds) == np.array(sample_true_labels))\nprint(f\"Baseline (zero-shot) accuracy on test samples: {baseline_accuracy:.2%}\")\nprint(f\"Our fine-tuned model accuracy: {test_results['eval_accuracy']:.2%}\")\nprint(f\"Improvement: {test_results['eval_accuracy'] - baseline_accuracy:+.2%}\")\n\n# ============================================================================\n# INFERENCE WITH EXPLANATIONS\n# ============================================================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\ndef predict_with_confidence(text, show_all_probs=False):\n    \"\"\"Predict emotion with confidence scores\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n    inputs = inputs.to(device)\n   \n    with torch.no_grad():\n        outputs = model(**inputs)\n   \n    probs = F.softmax(outputs.logits, dim=-1)[0]\n    pred_label = torch.argmax(probs).item()\n   \n    if show_all_probs:\n        print(f\"\\nText: {text}\")\n        print(\"Probabilities:\")\n        for i, prob in enumerate(probs):\n            print(f\" {EMOTION_LABELS[i]:10s}: {prob.item():.2%}\")\n        print(f\"‚Üí Predicted: {EMOTION_LABELS[pred_label]}\")\n   \n    return pred_label, probs[pred_label].item()\n\n# Test on diverse samples from test set\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INFERENCE ON DIVERSE TEST SAMPLES\")\nprint(\"=\" * 80)\n\n# Get samples from each emotion in test set\ntest_samples = []\nfor emotion_id in range(num_labels):\n    emotion_indices = np.where(true_labels == emotion_id)[0]\n    if len(emotion_indices) >= 2:\n        for idx in emotion_indices[:2]:\n            test_samples.append((test_texts[idx], emotion_id))\n\ncorrect_predictions = 0\ntotal_predictions = len(test_samples)\n\nprint(f\"Testing on {total_predictions} diverse samples from test set:\")\nfor text, true_label in test_samples:\n    pred_label, confidence = predict_with_confidence(text, show_all_probs=False)\n   \n    status = \"‚úÖ\" if pred_label == true_label else \"‚ùå\"\n    if pred_label == true_label:\n        correct_predictions += 1\n   \n    true_emotion = EMOTION_LABELS[true_label]\n    pred_emotion = EMOTION_LABELS[pred_label]\n   \n    print(f\"\\n{status} '{text[:70]}...'\")\n    print(f\" True: {true_emotion:10s} | Pred: {pred_emotion:10s} | Conf: {confidence:.1%}\")\n\nprint(f\"\\nDiverse sample accuracy: {correct_predictions}/{total_predictions} = {correct_predictions/total_predictions:.1%}\")\n\n# ============================================================================\n# SAVE MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAVING FINE-TUNED MODEL\")\nprint(\"=\" * 80)\n\ntrainer.save_model(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\n# Save configuration\nconfig_info = {\n    \"emotion_labels\": EMOTION_LABELS,\n    \"base_model\": MODEL_NAME,\n    \"fine_tuned_on\": \"4-emotion dataset\",\n    \"performance\": {\n        \"accuracy\": test_results['eval_accuracy'],\n        \"f1_macro\": test_results['eval_f1_macro'],\n        \"f1_weighted\": test_results['eval_f1_weighted']\n    }\n}\n\nwith open(f\"{OUTPUT_DIR}/final_model/model_config.json\", 'w') as f:\n    json.dump(config_info, f, indent=2)\n\nprint(f\"‚úì Model saved to: {OUTPUT_DIR}/final_model\")\nprint(f\"‚úì Model configuration saved\")\nprint(\"=\" * 80)\nprint(\"\\nüéâ FINE-TUNING COMPLETE!\")\nprint(f\"Final Test Accuracy: {test_results['eval_accuracy']:.2%}\")\nprint(f\"Final Test F1 (Macro): {test_results['eval_f1_macro']:.2%}\")\n\n# ============================================================================\n# PERFORMANCE SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PERFORMANCE SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"‚úì Achieved performance: {test_results['eval_accuracy']:.2%} accuracy\")\nprint(f\"‚úì Using Twitter-optimized model: {MODEL_NAME}\")\n\n# Find best and worst performing emotions\nf1_scores = {}\nfor emotion in EMOTION_LABELS.values():\n    key = f\"eval_f1_{emotion.lower()}\"\n    f1_scores[emotion] = test_results[key]\n\nbest_emotion = max(f1_scores, key=f1_scores.get)\nworst_emotion = min(f1_scores, key=f1_scores.get)\n\nprint(f\"\\nModel Strengths:\")\nprint(f\" - Best at detecting {best_emotion} (F1: {f1_scores[best_emotion]:.2%})\")\nprint(f\"\\nAreas for Improvement:\")\nprint(f\" - {worst_emotion} detection could be improved (F1: {f1_scores[worst_emotion]:.2%})\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:12:05.218931Z","iopub.execute_input":"2025-11-22T07:12:05.219819Z","iopub.status.idle":"2025-11-22T07:23:29.686796Z","shell.execute_reply.started":"2025-11-22T07:12:05.219771Z","shell.execute_reply":"2025-11-22T07:23:29.686044Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n================================================================================\nEMOTION DISTRIBUTION\n================================================================================\n0: Joy        -  9211 samples (26.27%)\n1: Sadness    -  9530 samples (27.18%)\n2: Neutral    -  6412 samples (18.29%)\n3: Anger      -  9913 samples (28.27%)\n\nTotal: 35066 samples\n================================================================================\n\n================================================================================\nSAMPLE TEXTS PER EMOTION\n================================================================================\n\nJOY (Label 0):\n 1. vene2ia great...\n 2. mrcartersnurse congratulations for your mom for tomorrow buenas noches...\n 3. ddlovato oh i see thanks for replying anyway how are you...\n\nSADNESS (Label 1):\n 1. just finished watching quotmarley and mequot...\n 2. msignorile weather sucks up here...\n 3. sasss09 hahaha sadly this ones supposed to be done individually so no chance of ...\n\nNEUTRAL (Label 2):\n 1. sadknob right now if be happy to win a packet of salt n vinegar crisps or a new ...\n 2. ok im out of here for now just popped in to say hi and check on things ill proba...\n 3. just finished my 1st new song soon on youtube keeping you updated...\n\nANGER (Label 3):\n 1. tanisha found herself in an outrageous situation...\n 2. ok free people skirt hide and seeks over...\n 3. the toys r us advert makes me want to scream...\n================================================================================\n\n================================================================================\nCLASS WEIGHTS (for imbalanced data)\n================================================================================\nJoy        (Label 0): 0.952\nSadness    (Label 1): 0.920\nNeutral    (Label 2): 1.367\nAnger      (Label 3): 0.884\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/35066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43d04b6a721d4a8d9c8f84e8baac8b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2b445b795d4c9597e9afdcf0fd7b5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d087ab40606452a8f519c9e627ab1b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcb38fab7f141e0bc4896e8ab1b9e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3881073a3e3b445f8edcc6fe30407864"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc9c6894c314957ade5fc0762ad73d2"}},"metadata":{}},{"name":"stdout","text":"\n‚úì Dataset sizes - Train: 28052 | Val: 3507 | Test: 3507\n\nValidation set distribution:\n Joy       :  921 (26.26%)\n Sadness   :  953 (27.17%)\n Neutral   :  642 (18.31%)\n Anger     :  991 (28.26%)\n\nLoading Twitter sentiment model: cardiffnlp/twitter-roberta-base-sentiment-latest\nThis model is pre-trained on Twitter data for sentiment analysis!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796603b344904e6482477ce44f180137"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([4]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úì Twitter sentiment model loaded successfully!\nOriginal model has 3 sentiment classes, adapted to our 4 emotion classes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea8e04edebe425f85b00a7b870e6b9d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/4018193939.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nFINE-TUNING TWITTER SENTIMENT MODEL FOR EMOTION CLASSIFICATION\nModel: cardiffnlp/twitter-roberta-base-sentiment-latest\nStrategy: Transfer Learning from Twitter Sentiment + Class Weights\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1756' max='2195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1756/2195 10:49 < 02:42, 2.70 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n      <th>F1 Weighted</th>\n      <th>F1 Joy</th>\n      <th>F1 Sadness</th>\n      <th>F1 Neutral</th>\n      <th>F1 Anger</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.582800</td>\n      <td>0.571646</td>\n      <td>0.774451</td>\n      <td>0.770122</td>\n      <td>0.772425</td>\n      <td>0.702187</td>\n      <td>0.756303</td>\n      <td>0.762115</td>\n      <td>0.859885</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.491000</td>\n      <td>0.572678</td>\n      <td>0.786997</td>\n      <td>0.783199</td>\n      <td>0.786972</td>\n      <td>0.716177</td>\n      <td>0.778503</td>\n      <td>0.759232</td>\n      <td>0.878882</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.395600</td>\n      <td>0.598695</td>\n      <td>0.780439</td>\n      <td>0.775032</td>\n      <td>0.778272</td>\n      <td>0.692818</td>\n      <td>0.773143</td>\n      <td>0.759166</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.322900</td>\n      <td>0.668960</td>\n      <td>0.783005</td>\n      <td>0.780011</td>\n      <td>0.783563</td>\n      <td>0.713805</td>\n      <td>0.772497</td>\n      <td>0.758414</td>\n      <td>0.875326</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nFINAL EVALUATION ON TEST SET\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Test Set Performance ---\nAccuracy: 0.7904\nF1 (Macro): 0.7873\nF1 (Weight): 0.7903\n\nPer-Emotion F1 Scores:\n Joy       : 0.7133\n Sadness   : 0.7742\n Neutral   : 0.7738\n Anger     : 0.8879\n\n================================================================================\nCONFUSION MATRIX\n================================================================================\n  Joy      Sadness  Neutral  Anger   \nJoy       :       612      152      121       36\nSadness   :        88      785       45       35\nNeutral   :        61       52      520        8\nAnger     :        34       86       17      855\n\n================================================================================\nDETAILED CLASSIFICATION REPORT\n================================================================================\n              precision    recall  f1-score   support\n\n         Joy     0.7698    0.6645    0.7133       921\n     Sadness     0.7302    0.8237    0.7742       953\n     Neutral     0.7397    0.8112    0.7738       641\n       Anger     0.9154    0.8619    0.8879       992\n\n    accuracy                         0.7904      3507\n   macro avg     0.7888    0.7903    0.7873      3507\nweighted avg     0.7947    0.7904    0.7903      3507\n\n\n================================================================================\nERROR ANALYSIS\n================================================================================\nTotal misclassified: 735 (21.0%)\n\nMost common misclassifications:\n Joy ‚Üí Sadness: 152 errors (20.7%)\n Joy ‚Üí Neutral: 121 errors (16.5%)\n Sadness ‚Üí Joy: 88 errors (12.0%)\n Anger ‚Üí Sadness: 86 errors (11.7%)\n Neutral ‚Üí Joy: 61 errors (8.3%)\n Neutral ‚Üí Sadness: 52 errors (7.1%)\n Sadness ‚Üí Neutral: 45 errors (6.1%)\n Joy ‚Üí Anger: 36 errors (4.9%)\n Sadness ‚Üí Anger: 35 errors (4.8%)\n Anger ‚Üí Joy: 34 errors (4.6%)\n\nExample misclassifications:\n\n 1. 'i have turned into a fast food whore...'\n    True: Joy ‚Üí Pred: Anger\n\n 2. 'hmmosaka last show todayvery sad i can decode ur msg haha cant wait till u get 2 sydney d i missed o...'\n    True: Neutral ‚Üí Pred: Sadness\n\n 3. 'torithompson u may need this pilli think u are crazyhere take on of mine...'\n    True: Anger ‚Üí Pred: Joy\n\n 4. 'having my hair dyed today ugh im bored still tired from friday lol swear down bossman...'\n    True: Joy ‚Üí Pred: Sadness\n\n 5. 'idk why im so hyper im jumping everyhere ugh lets let it be friday sweeney todd then cinco de mayo i...'\n    True: Sadness ‚Üí Pred: Anger\n\n================================================================================\nBASELINE COMPARISON: Zero-shot with original Twitter model\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Baseline (zero-shot) accuracy on test samples: 23.00%\nOur fine-tuned model accuracy: 79.04%\nImprovement: +56.04%\n\n================================================================================\nINFERENCE ON DIVERSE TEST SAMPLES\n================================================================================\nTesting on 8 diverse samples from test set:\n\n‚úÖ '3wordsaftersex my turn yet...'\n True: Joy        | Pred: Joy        | Conf: 81.1%\n\n‚ùå 'i have turned into a fast food whore...'\n True: Joy        | Pred: Anger      | Conf: 43.2%\n\n‚úÖ 'lulion07 im praying for you sorry to hear about your bro man...'\n True: Sadness    | Pred: Sadness    | Conf: 99.0%\n\n‚úÖ 'i feel sicklike dont wanna get out of bed be bothered dont go 2 work t...'\n True: Sadness    | Pred: Sadness    | Conf: 46.5%\n\n‚ùå 'hmmosaka last show todayvery sad i can decode ur msg haha cant wait ti...'\n True: Neutral    | Pred: Sadness    | Conf: 94.3%\n\n‚úÖ 'divinediva1 norwood house party haaaaaa yaaaaaaay smiles...'\n True: Neutral    | Pred: Neutral    | Conf: 98.2%\n\n‚úÖ 'gooooodnight i fully gave up on my english pride and prejudice love th...'\n True: Anger      | Pred: Anger      | Conf: 80.0%\n\n‚ùå 'torithompson u may need this pilli think u are crazyhere take on of mi...'\n True: Anger      | Pred: Joy        | Conf: 36.4%\n\nDiverse sample accuracy: 5/8 = 62.5%\n\n================================================================================\nSAVING FINE-TUNED MODEL\n================================================================================\n‚úì Model saved to: ./sentiment_classifier_cardiffnlp/final_model\n‚úì Model configuration saved\n================================================================================\n\nüéâ FINE-TUNING COMPLETE!\nFinal Test Accuracy: 79.04%\nFinal Test F1 (Macro): 78.73%\n\n================================================================================\nPERFORMANCE SUMMARY\n================================================================================\n‚úì Achieved performance: 79.04% accuracy\n‚úì Using Twitter-optimized model: cardiffnlp/twitter-roberta-base-sentiment-latest\n\nModel Strengths:\n - Best at detecting Anger (F1: 88.79%)\n\nAreas for Improvement:\n - Joy detection could be improved (F1: 71.33%)\n================================================================================\n","output_type":"stream"}],"execution_count":19}]}